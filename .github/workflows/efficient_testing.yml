name: OmniTarget Efficient Testing Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run production tests daily at 2 AM UTC
    - cron: '0 2 * * *'

jobs:
  # Fast development tests (run on every push/PR)
  development-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-xdist pytest-timeout
    
    - name: Run development tests
      run: |
        python run_efficient_tests.py --mode dev --verbose
    
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: development-test-results
        path: test-results/

  # Integration tests (run on PRs and develop branch)
  integration-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: development-tests
    if: github.event_name == 'pull_request' || github.ref == 'refs/heads/develop'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-xdist pytest-timeout
    
    - name: Run integration tests
      run: |
        python run_efficient_tests.py --mode integration --verbose --parallel
    
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: integration-test-results
        path: test-results/

  # Performance tests (run on develop branch and releases)
  performance-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    needs: integration-tests
    if: github.ref == 'refs/heads/develop' || github.event_name == 'schedule'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-xdist pytest-timeout psutil
    
    - name: Run performance tests
      run: |
        python run_efficient_tests.py --mode performance --verbose
    
    - name: Upload performance results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: performance-test-results
        path: test-results/

  # Production tests (run on releases and scheduled)
  production-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 120
    needs: performance-tests
    if: github.event_name == 'schedule' || startsWith(github.ref, 'refs/tags/')
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-xdist pytest-timeout psutil
    
    - name: Check MCP server availability
      run: |
        python run_efficient_tests.py --check-mcp
    
    - name: Run production tests
      run: |
        python run_efficient_tests.py --mode production --verbose
      env:
        MCP_SERVER_TIMEOUT: 300
        MCP_SERVER_RETRIES: 3
    
    - name: Upload production test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: production-test-results
        path: test-results/

  # Test summary and reporting
  test-summary:
    runs-on: ubuntu-latest
    needs: [development-tests, integration-tests, performance-tests, production-tests]
    if: always()
    
    steps:
    - name: Download all test results
      uses: actions/download-artifact@v3
      with:
        path: test-results/
    
    - name: Generate test summary
      run: |
        echo "# Test Summary" > test-summary.md
        echo "" >> test-summary.md
        echo "## Development Tests" >> test-summary.md
        if [ -d "test-results/development-test-results" ]; then
          echo "✅ Development tests completed" >> test-summary.md
        else
          echo "❌ Development tests failed" >> test-summary.md
        fi
        echo "" >> test-summary.md
        echo "## Integration Tests" >> test-summary.md
        if [ -d "test-results/integration-test-results" ]; then
          echo "✅ Integration tests completed" >> test-summary.md
        else
          echo "❌ Integration tests failed" >> test-summary.md
        fi
        echo "" >> test-summary.md
        echo "## Performance Tests" >> test-summary.md
        if [ -d "test-results/performance-test-results" ]; then
          echo "✅ Performance tests completed" >> test-summary.md
        else
          echo "❌ Performance tests failed" >> test-summary.md
        fi
        echo "" >> test-summary.md
        echo "## Production Tests" >> test-summary.md
        if [ -d "test-results/production-test-results" ]; then
          echo "✅ Production tests completed" >> test-summary.md
        else
          echo "❌ Production tests failed" >> test-summary.md
        fi
    
    - name: Upload test summary
      uses: actions/upload-artifact@v3
      with:
        name: test-summary
        path: test-summary.md

  # Security and quality checks
  security-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install security tools
      run: |
        pip install bandit safety
    
    - name: Run security scan
      run: |
        bandit -r src/ -f json -o bandit-report.json || true
        safety check --json --output safety-report.json || true
    
    - name: Upload security reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-reports
        path: |
          bandit-report.json
          safety-report.json

  # Code quality and coverage
  code-quality:
    runs-on: ubuntu-latest
    timeout-minutes: 20
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install quality tools
      run: |
        pip install flake8 black isort mypy coverage pytest-cov
    
    - name: Run code quality checks
      run: |
        # Code formatting
        black --check src/ tests/ || echo "Code formatting issues found"
        
        # Import sorting
        isort --check-only src/ tests/ || echo "Import sorting issues found"
        
        # Linting
        flake8 src/ tests/ --max-line-length=100 --ignore=E203,W503 || echo "Linting issues found"
        
        # Type checking
        mypy src/ --ignore-missing-imports || echo "Type checking issues found"
    
    - name: Run coverage analysis
      run: |
        pytest tests/unit/ tests/fast/ --cov=src --cov-report=xml --cov-report=html
      continue-on-error: true
    
    - name: Upload coverage reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: coverage-reports
        path: |
          coverage.xml
          htmlcov/

  # Notification on failure
  notify-failure:
    runs-on: ubuntu-latest
    needs: [development-tests, integration-tests, performance-tests, production-tests]
    if: failure()
    
    steps:
    - name: Notify on test failure
      run: |
        echo "❌ Tests failed in OmniTarget pipeline"
        echo "Check the workflow logs for details"
        # Add notification logic here (Slack, email, etc.)
